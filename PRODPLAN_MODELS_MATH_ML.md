# ProdPlan 4.0 - Modelos Matem√°ticos e Machine Learning

## üìã Resumo Executivo

O ProdPlan 4.0 utiliza uma combina√ß√£o de:
- **Otimiza√ß√£o Matem√°tica**: MILP, CP-SAT, heur√≠sticas
- **Machine Learning**: Forecasting, Deep Learning (CVAE), Reinforcement Learning (stubs)
- **Estat√≠stica Bayesiana**: Estima√ß√£o de incerteza, Monte Carlo
- **Infer√™ncia Causal**: Grafos causais, estima√ß√£o de efeitos

---

## üè≠ 1. SCHEDULING (Planeamento de Produ√ß√£o)

### 1.1 Heur√≠sticas de Dispatching
**Ficheiro:** `backend/scheduler.py`

```
Regras implementadas:
- FIFO: First In, First Out
- SPT: Shortest Processing Time
- EDD: Earliest Due Date (via priority + due_date sorting)

Algoritmo:
1. Ordenar ordens por (priority DESC, due_date ASC)
2. Para cada opera√ß√£o:
   start_time = max(machine_free, order_last_finish, prev_op_finish)
   end_time = start_time + duration
3. Atualizar machine_next_free e order_last_finish
```

### 1.2 MILP (Mixed-Integer Linear Programming)
**Ficheiro:** `backend/optimization/scheduling_models.py`

**Formula√ß√£o Job-Shop:**
```
Vari√°veis:
- x[op,m] ‚àà {0,1}: opera√ß√£o op atribu√≠da √† m√°quina m
- s[op] ‚àà ‚Ñù‚Å∫: tempo de in√≠cio da opera√ß√£o
- e[op] ‚àà ‚Ñù‚Å∫: tempo de fim da opera√ß√£o
- C_max ‚àà ‚Ñù‚Å∫: makespan

Restri√ß√µes:
1. Atribui√ß√£o √∫nica: Œ£_m x[op,m] = 1, ‚àÄ op
2. Tempo de processamento: e[op] ‚â• s[op] + p[op,m] - M(1 - x[op,m])
3. Preced√™ncia: s[succ] ‚â• e[pred], ‚àÄ (pred,succ) ‚àà Prec
4. N√£o-sobreposi√ß√£o (Big-M):
   e[op1] ‚â§ s[op2] + M(1-y) + M(2-x[op1,m]-x[op2,m])
   e[op2] ‚â§ s[op1] + My + M(2-x[op1,m]-x[op2,m])
5. Makespan: C_max ‚â• e[op], ‚àÄ op

Objetivo: min C_max (ou soma ponderada)
Solver: OR-Tools CBC/SCIP
```

### 1.3 CP-SAT (Constraint Programming)
**Ficheiro:** `backend/optimization/scheduling_models.py`

**Formula√ß√£o:**
```
Vari√°veis:
- interval[op]: vari√°vel de intervalo (start, duration, end)
- machine[op]: √≠ndice da m√°quina atribu√≠da
- optional_interval[op,m]: intervalo opcional por m√°quina

Restri√ß√µes Globais:
- NoOverlap(intervals_on_machine): disjun√ß√£o de intervalos
- Cumulative: para m√°quinas com capacidade > 1
- AddExactlyOne(presence_vars): exatamente uma m√°quina

Propaga√ß√£o: Constraint propagation + SAT learning
Solver: OR-Tools CP-SAT
```

### 1.4 DRL (Deep Reinforcement Learning) - Stub
**Ficheiro:** `backend/optimization/drl_scheduler.py`

```
MDP Formulation:
- State: (machine_states, operation_states, global_time)
- Action: operation_index to dispatch
- Reward: -tardiness - makespan_factor - setup_penalty + flow_bonus

Algoritmo: PPO/A2C/DQN (Stable-Baselines3)
Status: Stub para R&D
```

---

## üìà 2. FORECASTING (Previs√£o de Demanda)

**Ficheiro:** `backend/ml/forecasting.py`

### 2.1 Naive Forecaster
```
≈∑_t+h = y_t (√∫ltimo valor observado)
Intervalo: ≈∑ ¬± z * œÉ (œÉ = desvio padr√£o hist√≥rico)
```

### 2.2 Moving Average
```
≈∑_t+h = (1/k) Œ£_{i=0}^{k-1} y_{t-i}
Intervalo: MA ¬± z * œÉ_window
k = window_size (default 7)
```

### 2.3 ETS (Exponential Smoothing)
```
Holt's Linear Method:
- N√≠vel: L_t = Œ± * y_t + (1-Œ±) * (L_{t-1} + T_{t-1})
- Tend√™ncia: T_t = Œ≤ * (L_t - L_{t-1}) + (1-Œ≤) * T_{t-1}
- Previs√£o: ≈∑_{t+h} = L_t + h * T_t

Par√¢metros: Œ±=0.3, Œ≤=0.1
Biblioteca: statsmodels.tsa.holtwinters
```

### 2.4 ARIMA
```
ARIMA(p,d,q):
- AR(p): y_t = c + Œ£_{i=1}^{p} œÜ_i * y_{t-i} + Œµ_t
- I(d): diferencia√ß√£o d vezes
- MA(q): Œµ_t = Œ£_{j=1}^{q} Œ∏_j * Œµ_{t-j}

Default: ARIMA(1,1,1)
Intervalos: get_forecast().conf_int(alpha=0.05)
Biblioteca: statsmodels.tsa.arima.model
```

### 2.5 XGBoost
```
Features: Lag features (√∫ltimos n_lags valores)
Modelo: XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1)
Predi√ß√£o recursiva: usar predi√ß√µes anteriores como input
```

### 2.6 Transformer (Stub)
```
Arquitetura planejada:
- Temporal Fusion Transformer (TFT)
- Non-Stationary Transformers
Fallback atual: ETS
```

---

## ü§ñ 3. DIGITAL TWIN (Health Indicators + RUL)

### 3.1 CVAE (Conditional Variational Autoencoder)
**Ficheiro:** `backend/digital_twin/health_indicator_cvae.py`

**Arquitetura PyTorch:**
```
Input: x ‚àà ‚Ñù^64 (sensor features) + context (machine, op, product embeddings)

Encoder:
- x_concat = [x; emb_machine; emb_op; emb_product] ‚àà ‚Ñù^88
- h = LeakyReLU(BatchNorm(Linear(128))) ‚Üí ... ‚Üí ‚Ñù^32
- Œº = Linear(32 ‚Üí 16)
- log œÉ¬≤ = Linear(32 ‚Üí 16)

Reparametrization:
- Œµ ~ N(0, I)
- z = Œº + œÉ * Œµ

Decoder:
- z_concat = [z; emb_machine; emb_op; emb_product] ‚àà ‚Ñù^40
- h = LeakyReLU(BatchNorm(Linear(...)))
- xÃÇ = Linear(h ‚Üí 64)

Health Indicator Head:
- HI = Sigmoid(Linear(ReLU(Linear(z))))
- HI ‚àà [0, 1]: 1 = saud√°vel, 0 = cr√≠tico

Loss Function:
- L = L_recon + Œ≤ * KL(q(z|x,c) || p(z)) + L_HI
- L_recon = MSE(x, xÃÇ)
- KL = -0.5 * Œ£(1 + log œÉ¬≤ - Œº¬≤ - œÉ¬≤)
```

### 3.2 RUL Estimator (Remaining Useful Life)
**Ficheiro:** `backend/digital_twin/rul_estimator.py`

**Modelo Exponencial:**
```
Degrada√ß√£o: HI(t) = HI_0 * exp(-Œªt)

Fitting (Least Squares):
- log(HI) = log(HI_0) - Œªt
- Œª = -slope (regress√£o linear em log-scale)

RUL: T_RUL = ln(HI_current / HI_fail) / Œª

Incerteza:
- œÉ_RUL ‚âà RMSE / (Œª * HI) * T_RUL * 0.3
- IC 95%: [T_RUL - 1.96*œÉ, T_RUL + 1.96*œÉ]
```

**Modelo Linear:**
```
Degrada√ß√£o: HI(t) = HI_0 - Œªt
RUL: T_RUL = (HI_current - HI_fail) / Œª
```

**Monte Carlo (Gaussian Process Approximation):**
```
Para N=1000 amostras:
1. Perturbar Œª: Œª' = Œª * N(1, 0.2)
2. Perturbar HI: HI' = HI + N(0, 0.05)
3. Calcular RUL': ln(HI' / HI_fail) / Œª'
4. Estat√≠sticas: mean, std, percentis 2.5% e 97.5%
```

### 3.3 RUL-Integrated Scheduling
**Ficheiro:** `backend/digital_twin/rul_integration_scheduler.py`

```
Penaliza√ß√£o de M√°quinas:
- Se RUL < threshold: penalty = 1 - (RUL / threshold)
- Penalty range: [0, 1]

Ajuste do Plano:
1. Calcular HI e RUL para cada m√°quina
2. M√°quinas com status CRITICAL ou WARNING:
   - Redistribuir opera√ß√µes para m√°quinas alternativas
   - Sugerir manuten√ß√£o preventiva
   - Reduzir carga em opera√ß√µes longas
3. Gerar RULAdjustedPlan com decis√µes documentadas
```

---

## üõ°Ô∏è 4. ZDM (Zero Disruption Manufacturing)

### 4.1 Gera√ß√£o de Cen√°rios de Falha
**Ficheiro:** `backend/simulation/zdm/failure_scenario_generator.py`

```
Tipos de Falha:
- SUDDEN: P(failure) ~ Exponential(Œª_RUL)
- GRADUAL: degradation_rate = 1 - RUL_normalized
- QUALITY: reject_rate ~ Beta(Œ±, Œ≤)
- MATERIAL: P(shortage) baseado em ROP
- OPERATOR: P(absence) ~ hist√≥rico

Par√¢metros por Tipo:
- duration_hours ~ LogNormal(Œº, œÉ)
- severity ‚àà [0, 1]
- quality_reject_rate ‚àà [0, 0.5]
```

### 4.2 Simula√ß√£o de Resili√™ncia
**Ficheiro:** `backend/simulation/zdm/zdm_simulator.py`

```
M√©tricas de Impacto:
- downtime_hours: dura√ß√£o da paragem
- operations_delayed: # opera√ß√µes afetadas
- throughput_loss_pct = (ops_delayed / total_ops) * 100 * severity
- otd_impact_pct = (orders_at_risk / orders_impacted) * 100
- estimated_cost = downtime * ‚Ç¨500/h + orders_at_risk * ‚Ç¨200

Severity Score:
score = 0.3 * time_score + 0.3 * throughput_score + 0.4 * otd_score

Resilience Score = 100 - avg(severity_scores)
```

### 4.3 Estrat√©gias de Recupera√ß√£o
**Ficheiro:** `backend/simulation/zdm/recovery_strategy_engine.py`

```
Estrat√©gias:
1. REROUTE: reencaminhar para m√°quinas alternativas
   - recovery_factor += 0.4 * rerouting_efficiency
2. ADD_OVERTIME: adicionar horas extra
   - recovery_factor += 0.3 * min(1, max_overtime / duration)
3. PRIORITY_SHUFFLE: repriorizar ordens VIP
   - recovery_factor += 0.2

Recovery Status:
- SUCCESS: recovery_factor >= 0.8
- PARTIAL: 0.4 <= recovery_factor < 0.8
- FAILED: recovery_factor < 0.4
```

---

## üì¶ 5. SMART INVENTORY (ROP Din√¢mico)

### 5.1 ROP Cl√°ssico
**Ficheiro:** `backend/smart_inventory/rop_engine.py`

```
F√≥rmula:
ROP = Œº_d * L + z * œÉ_d * ‚àöL

onde:
- Œº_d = consumo m√©dio di√°rio (do forecast)
- œÉ_d = desvio padr√£o do consumo
- L = lead time (dias)
- z = quantil do n√≠vel de servi√ßo (z_0.95 = 1.96)

Safety Stock:
SS = z * œÉ_d * ‚àöL

ROP Din√¢mico (com sazonalidade):
ROP = Œº_d(t) * L + z * œÉ_d(t) * ‚àöL + seasonal_adjustment(t)
```

### 5.2 Risco de Ruptura (Monte Carlo)
```
Simula√ß√£o:
1. Gerar N=10000 amostras de consumo di√°rio
   D_i ~ N(Œº_d, œÉ_d), truncado em 0
2. Stock ap√≥s 30 dias: S_30 = S_0 - Œ£_{j=1}^{30} D_j
3. P(ruptura) = #{S_30 < 0} / N

Aproxima√ß√£o Anal√≠tica:
- E[consumo_30d] = Œº_d * 30
- Var[consumo_30d] = œÉ_d¬≤ * 30
- P(ruptura) = Œ¶((0 - (S_0 - Œº*30)) / (œÉ*‚àö30))
```

---

## üîó 6. CAUSAL ANALYSIS (Infer√™ncia Causal)

### 6.1 Grafo Causal
**Ficheiro:** `backend/causal/causal_graph_builder.py`

```
Estrutura DAG (Directed Acyclic Graph):
- N√≥s: vari√°veis (treatments, outcomes, confounders)
- Arestas: rela√ß√µes causais com strength e confidence

Vari√°veis de Tratamento:
- setup_frequency, batch_size, machine_load
- night_shifts, overtime_hours, maintenance_delay

Outcomes:
- energy_cost, makespan, tardiness, otd_rate
- machine_wear, failure_prob, operator_stress

Confounders:
- demand_volume, product_mix, seasonality
- machine_age, workforce_experience
```

### 6.2 Estima√ß√£o de Efeitos Causais
**Ficheiro:** `backend/causal/causal_effect_estimator.py`

**Regression Adjustment:**
```
E[Y|do(T)] ‚âà E[Y|T, Z]

Modelo OLS:
Y = Œ≤_0 + Œ≤_1*T + Œ≤_2*Z_1 + ... + Œ≤_n*Z_n + Œµ

Efeito Causal (ATE) = Œ≤_1

Estima√ß√£o:
Œ≤ = (X'X)^{-1} X'y  (least squares)
Var(Œ≤) = œÉ¬≤ * (X'X)^{-1}
œÉ¬≤ = ||residuals||¬≤ / (n - k)

Intervalo de Confian√ßa:
Œ≤_1 ¬± t_{0.975, n-k} * SE(Œ≤_1)

P-value:
t = Œ≤_1 / SE(Œ≤_1)
p = 2 * (1 - CDF_t(|t|, n-k))
```

**Identifica√ß√£o de Confounders (Backdoor Criterion):**
```
Algoritmo:
1. Ancestrais do treatment: A_T
2. Pais do outcome (excluindo treatment): P_Y
3. Confounders = {v : v.type == CONFOUNDER ‚àß (v ‚àà A_T ‚à® v ‚àà Ancestrais(outcome))}
```

### 6.3 Complexity Dashboard
**Ficheiro:** `backend/causal/complexity_dashboard_engine.py`

```
M√©tricas de Complexidade:
- n_variables: n√∫mero de vari√°veis no grafo
- n_relations: n√∫mero de rela√ß√µes causais
- connectivity: n_relations / (n_variables * (n_variables - 1))
- avg_path_length: comprimento m√©dio de caminhos causais
- complexity_score = f(n_vars, n_rels, connectivity, path_length)

Identifica√ß√£o de Trade-offs:
Para cada treatment:
  effects = estimate_all_effects(treatment)
  positive = [e for e in effects if e.direction == "positive"]
  negative = [e for e in effects if e.direction == "negative"]
  if positive and negative:
    trade_off = (treatment, positive, negative)

Leverage Points:
Variables com alto impacto em m√∫ltiplos outcomes positivos
```

---

## üìä 7. M√âTRICAS E KPIs

### 7.1 M√©tricas de Scheduling
```
Makespan: C_max = max(end_time[op]) - min(start_time[op])
Tardiness: T = Œ£ max(0, end_time[order] - due_date[order])
OTD Rate: % de ordens entregues a tempo
Utiliza√ß√£o: Œ£ duration[m] / (C_max * n_machines) * 100%
Setup Time: Œ£ setup_time (quando op_code muda)
```

### 7.2 M√©tricas de Forecasting
```
MAPE: (1/n) Œ£ |y - ≈∑| / |y| * 100
RMSE: ‚àö((1/n) Œ£ (y - ≈∑)¬≤)
MAE: (1/n) Œ£ |y - ≈∑|
SNR (Signal-to-Noise): Œº / œÉ (forecastability)
```

### 7.3 M√©tricas de Digital Twin
```
Health Index: HI ‚àà [0, 1]
RUL: horas at√© falha (com IC 95%)
Status: HEALTHY (HI > 0.7), DEGRADED (0.5-0.7), WARNING (0.3-0.5), CRITICAL (< 0.3)
```

### 7.4 M√©tricas de Resili√™ncia (ZDM)
```
Resilience Score: 100 - avg(severity_scores)
Recovery Rate: #SUCCESS / #scenarios * 100%
Avg Recovery Time: m√©dia de horas para recuperar
Avg Throughput Loss: m√©dia de % de perda
```

---

## üß™ 8. ALGORITMOS R&D (Stubs/TODO)

### 8.1 Deep Bayesian RUL
```
TODO: MC Dropout, HMC/VI para incerteza epist√©mica
Objetivo: Separar incerteza aleat√≥ria vs epist√©mica
```

### 8.2 Transformer Forecasting
```
TODO: Temporal Fusion Transformer
Objetivo: Superar ARIMA em s√©ries n√£o-estacion√°rias
```

### 8.3 MILP vs CP-SAT Benchmarks
```
TODO: Comparar solution quality, time, optimality gap
Hip√≥tese: MILP melhor para < 100 ops, CP-SAT para > 100
```

### 8.4 DRL Scheduling
```
TODO: PPO com reward shaping
Objetivo: Generalizar para diferentes inst√¢ncias
```

### 8.5 DoWhy/EconML Integration
```
TODO: Double ML para CATE
Objetivo: Efeitos heterog√©neos por contexto
```

---

## üìö Refer√™ncias

1. Pinedo, M. (2016). *Scheduling: Theory, Algorithms, and Systems*
2. Hyndman & Athanasopoulos (2021). *Forecasting: Principles and Practice*
3. Kingma & Welling (2014). *Auto-Encoding Variational Bayes*
4. Pearl, J. (2009). *Causality: Models, Reasoning, and Inference*
5. Wu et al. (2022). *Non-stationary Transformers*
6. Google OR-Tools Documentation



