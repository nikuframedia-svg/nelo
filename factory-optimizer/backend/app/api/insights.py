"""API endpoints para o Insight Engine."""

import json
import logging
import re
from typing import Literal, Optional

from fastapi import APIRouter, HTTPException, Query
from fastapi.responses import JSONResponse

from app.insights.engine import InsightEngine
from app.insights.cache import get_insight_cache
from app.llm import LLMUnavailableError, LocalLLM
from app.llm.validator import validate_llm_output
from app.llm.industrial_validator import IndustrialLLMValidator
from app.insights.prompts import SYSTEM_PROMPT, get_prompt_by_mode
from app.etl.loader import get_loader

logger = logging.getLogger(__name__)
router = APIRouter()


@router.get("/context")
async def get_insight_context(
    mode: Literal["planeamento", "gargalos", "inventario", "resumo", "sugestoes"] = Query(
        "resumo", description="Modo de contexto"
    ),
    batch_id: Optional[str] = Query(None, description="Batch ID (opcional, usa o mais recente se n√£o fornecido)"),
):
    """Retorna contexto estruturado por modo."""
    try:
        engine = InsightEngine()
        context = engine.build_context_by_mode(mode)
        return context
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"Falha ao gerar contexto: {exc}") from exc


@router.get("/action-candidates")
async def get_action_candidates(
    batch_id: Optional[str] = Query(None, description="Batch ID (opcional)"),
):
    """
    Retorna ActionCandidates estruturados (sem LLM).
    Frontend pode usar diretamente para renderizar cards ou passar para LLM.
    """
    try:
        engine = InsightEngine()
        candidates = engine.build_action_candidates()
        
        # Formatar como cards estruturados
        cards = []
        for candidate in candidates:
            # Gerar t√≠tulo baseado no tipo
            tipo = candidate.get("tipo", "")
            alvo = candidate.get("alvo", "")
            titulo = ""
            
            if tipo == "desvio_carga":
                alternativa = candidate.get("alternativa", "")
                pct = candidate.get("pct_desvio", 0.0)
                titulo = f"Desviar {pct}% de carga de {alvo} para {alternativa}"
            elif tipo == "reposicao_stock":
                sku = candidate.get("sku", alvo)
                qty = candidate.get("qty_repor", 0)
                titulo = f"Repor {qty} unidades do SKU {sku}"
            elif tipo == "preventiva":
                titulo = f"Agendar manuten√ß√£o preventiva em {alvo}"
            elif tipo == "colar_familias":
                titulo = f"Colar fam√≠lias no {alvo}"
            elif tipo == "ajuste_overlap":
                titulo = f"Aumentar overlap no setor {alvo}"
            elif tipo == "reducao_excesso":
                sku = candidate.get("sku", alvo)
                titulo = f"Reduzir stock excessivo do SKU {sku}"
            else:
                titulo = f"A√ß√£o: {tipo} em {alvo}"
            
            cards.append({
                "acao": tipo,
                "titulo": titulo,
                "dados_base": candidate.get("dados_base", {}),
                "impacto_estimado": candidate.get("impacto_estimado", {}),
                "prioridade": candidate.get("prioridade", "BAIXO"),
                "alvo": alvo,
                "gargalo_afetado": candidate.get("gargalo_afetado"),
                "alternativa": candidate.get("alternativa"),
                "sku": candidate.get("sku"),
            })
        
        return {
            "count": len(cards),
            "cards": cards,
            "batch_id": batch_id,
        }
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"Falha ao gerar action candidates: {exc}") from exc


@router.get("/generate")
async def generate_insight(
    mode: Literal["planeamento", "gargalos", "inventario", "resumo", "sugestoes"] = Query(
        "resumo", description="Modo de gera√ß√£o"
    ),
    batch_id: Optional[str] = Query(None, description="Batch ID (opcional, usa o mais recente se n√£o fornecido)"),
    user_message: str = Query("", description="Mensagem opcional do utilizador"),
):
    """
    Gera insight LLM baseado no contexto do modo.
    Usa cache por batch_id+mode para evitar chamadas LLM desnecess√°rias.
    """
    try:
        # Obter batch_id atual se n√£o fornecido
        if not batch_id:
            loader = get_loader()
            status = loader.get_status()
            # Tentar obter batch_id mais recente do status
            batch_id = status.get("latest_batch_id") or status.get("batch_id") or "default"
        
        # Verificar cache
        cache = get_insight_cache()
        cached_insight = cache.get(batch_id, mode)
        
        if cached_insight and not user_message:
            # Validar cache antes de devolver (especialmente para planeamento)
            if mode == "planeamento":
                # Verificar se o cache tem estrutura antiga ou placeholders
                has_old_structure = any(
                    pattern in cached_insight.lower()
                    for pattern in ["resumo executivo", "1Ô∏è‚É£", "2Ô∏è‚É£", "3Ô∏è‚É£", "üí∞ Œî", "üìà overlap", "a f√°brica opera com poucas ordens"]
                )
                has_placeholders = any(
                    re.search(pattern, cached_insight, re.IGNORECASE)
                    for pattern in [r'\b(X|Y|Z|W|V|U)\s*(h|%|horas|unidades|pp)\b', r'\b[X-Z]h\b', r'\b[X-Z]\s*%\b']
                )
                starts_wrong = not cached_insight.strip().lower().startswith("nesta demonstra√ß√£o")
                # Verificar se n√£o cont√©m o novo texto esperado
                has_new_text = "an√°lise industrial do motor nikufra ops" in cached_insight.lower() or "estudo do plano antes e depois" in cached_insight.lower()
                
                if has_old_structure or has_placeholders or starts_wrong or not has_new_text:
                    # Cache inv√°lido - invalidar e regenerar
                    logger.warning(f"Cache inv√°lido detectado para {batch_id}:{mode}. Invalidando e regenerando.")
                    cache.invalidate(batch_id, mode)
                    cached_insight = None
            
            if cached_insight:
                # Retornar do cache se v√°lido
                logger.info(f"Cache hit para {batch_id}:{mode}")
                return {
                    "mode": mode,
                    "answer": cached_insight,
                    "insight": cached_insight,  # Campo adicional para compatibilidade com frontend
                    "model": "cached",
                    "batch_id": batch_id,
                    "cached": True,
                }
        
        # Cache miss ou user_message presente - gerar novo insight
        logger.info(f"Cache miss para {batch_id}:{mode}, gerando novo insight")
        
        try:
            engine = InsightEngine()
            context = engine.build_context_by_mode(mode)
            context_json = json.dumps(context, ensure_ascii=False, indent=2)
        except Exception as exc:
            logger.exception(f"Erro ao construir contexto para {mode}")
            raise HTTPException(status_code=500, detail=f"Erro ao construir contexto: {str(exc)}") from exc

        try:
            prompt = get_prompt_by_mode(mode, context_json, user_question=user_message if user_message else None)
            full_prompt = f"{SYSTEM_PROMPT}\n\n{prompt}"
        except Exception as exc:
            logger.exception(f"Erro ao construir prompt para {mode}")
            raise HTTPException(status_code=500, detail=f"Erro ao construir prompt: {str(exc)}") from exc

        if user_message:
            full_prompt += f"\n\nPergunta do utilizador: {user_message}"

        # Temperaturas e max_tokens por modo (otimizados)
        temperature_map = {
            "planeamento": 0.25,
            "gargalos": 0.25,
            "inventario": 0.25,
            "resumo": 0.3,
            "sugestoes": 0.25,
        }
        max_tokens_map = {
            "planeamento": 400,
            "gargalos": 350,
            "inventario": 350,
            "resumo": 300,
            "sugestoes": 400,
        }
        
        temperature = temperature_map.get(mode, 0.3)
        max_tokens = max_tokens_map.get(mode, 350)
        num_ctx = 2048  # Reduzido de 4096 para melhor performance

        llm = LocalLLM()
        try:
            answer_raw = llm.generate(
                prompt=full_prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                num_ctx=num_ctx
            )
            answer = answer_raw.strip()
        except LLMUnavailableError:
            return JSONResponse({"detail": "Modelo offline ‚Äî iniciar Ollama."}, status_code=200)

        # Validar output do LLM
        try:
            loader = get_loader()
            valid_skus = []
            valid_resources = []
            
            # Extrair SKUs e recursos v√°lidos do contexto
            if mode == "inventario":
                try:
                    inventory_data = loader.get_inventory_insights()
                    valid_skus = [sku.get("sku", "") for sku in inventory_data.get("skus", [])] if inventory_data else []
                except Exception as exc:
                    logger.warning(f"Erro ao obter inventory insights: {exc}")
                    valid_skus = []
            elif mode in ["gargalos", "planeamento"]:
                try:
                    # Para planeamento, extrair recursos do contexto de planeamento
                    if mode == "planeamento":
                        # Extrair recursos do contexto de planeamento
                        gargalo_principal = context.get("gargalo_principal", {})
                        gargalo_id = str(gargalo_principal.get("id", "")).replace("M-", "").replace("M", "").strip()
                        if gargalo_id and gargalo_id.isdigit():
                            valid_resources = [gargalo_id]
                        
                        # Tamb√©m buscar recursos dos roteiros e opera√ß√µes
                        roteiros = loader.get_roteiros()
                        if roteiros is not None and not roteiros.empty and "maquinas_possiveis" in roteiros.columns:
                            all_resources = set()
                            for machines in roteiros["maquinas_possiveis"].dropna():
                                if isinstance(machines, str):
                                    # Aceitar tanto "M-27" quanto "27"
                                    resources = re.findall(r'M[-\s]?(\d+)', machines)
                                    all_resources.update(resources)  # Guardar sem prefixo M-
                            valid_resources.extend(list(all_resources))
                            valid_resources = list(set(valid_resources))  # Remover duplicados
                    else:
                        # Para gargalos, usar a l√≥gica original
                        bottlenecks_insights = engine._extract_bottlenecks_insights()
                        valid_resources = [r.get("recurso", "") for r in bottlenecks_insights.get("top_resources", [])] if bottlenecks_insights else []
                        # Normalizar recursos (remover M- se existir)
                        valid_resources = [str(r).replace("M-", "").replace("M", "").strip() for r in valid_resources if r]
                        # Tamb√©m buscar recursos dos roteiros
                        roteiros = loader.get_roteiros()
                        if roteiros is not None and not roteiros.empty and "maquinas_possiveis" in roteiros.columns:
                            all_resources = set()
                            for machines in roteiros["maquinas_possiveis"].dropna():
                                if isinstance(machines, str):
                                    resources = re.findall(r'M[-\s]?(\d+)', machines)
                                    all_resources.update(resources)  # Guardar sem prefixo M-
                            valid_resources.extend(list(all_resources))
                            valid_resources = list(set(valid_resources))  # Remover duplicados
                except Exception as exc:
                    logger.warning(f"Erro ao obter recursos para valida√ß√£o: {exc}")
                    valid_resources = []
        except Exception as exc:
            logger.warning(f"Erro ao obter dados para valida√ß√£o: {exc}")
            valid_skus = []
            valid_resources = []
        
        # Valida√ß√£o dupla: validador original + validador industrial enterprise
        try:
            validation = validate_llm_output(answer, context, mode, valid_skus, valid_resources)
        except Exception as exc:
            logger.warning(f"Erro na valida√ß√£o b√°sica: {exc}")
            validation = {"is_valid": True, "errors": [], "warnings": [], "sanitized_text": answer, "should_regenerate": False}
        
        # Valida√ß√£o industrial rigorosa (n√≠vel enterprise)
        try:
            industrial_validator = IndustrialLLMValidator()
            action_candidates = context.get("actions", []) if mode == "sugestoes" else None
            industrial_validation = industrial_validator.validate(
                answer, context, mode, valid_skus, valid_resources, action_candidates
            )
        except Exception as exc:
            logger.warning(f"Erro na valida√ß√£o industrial: {exc}")
            industrial_validation = {"is_valid": True, "errors": [], "warnings": [], "sanitized_text": answer, "should_regenerate": False}
        
        # Combinar valida√ß√µes
        all_errors = validation.get("errors", []) + industrial_validation.get("errors", [])
        all_warnings = validation.get("warnings", []) + industrial_validation.get("warnings", [])
        is_valid = validation.get("is_valid", False) and industrial_validation.get("is_valid", False)
        should_regenerate = validation.get("should_regenerate", False) or industrial_validation.get("should_regenerate", False)
        
        # Se houver erros cr√≠ticos ou estrutura antiga, tentar regenerar
        if not is_valid and (should_regenerate or len(all_errors) > 0):
            # Prompt mais rigoroso para planeamento
            if mode == "planeamento":
                stricter_prompt = full_prompt + "\n\n‚ö†Ô∏è REGRAS ABSOLUTAS CR√çTICAS:\n- O resumo DEVE come√ßar EXATAMENTE com: 'Nesta demonstra√ß√£o, o sistema identificou um gargalo estrutural claro no recurso...'\n- N√ÉO uses placeholders gen√©ricos (X, Y, Z, W, V, U).\n- N√ÉO uses a estrutura antiga ('Resumo Executivo', '1Ô∏è‚É£ Plano Antes', etc.).\n- Segue EXATAMENTE o exemplo fornecido no prompt (par√°grafos corridos, sem bullets).\n- Podes mencionar recursos pelo n√∫mero (ex: 27, 29, 248), desde que estejam no contexto.\n- Podes mencionar: gargalo, setup, fam√≠lias, sequ√™ncia, ordens, carga, capacidade, produ√ß√£o, opera√ß√µes, turnos.\n- √â PROIBIDO mencionar: stock, invent√°rio, ABC, XYZ, ROP, risco, compras.\n- Se n√£o tens um valor real, N√ÉO o inventes.\n- O resumo deve terminar com: 'Este resumo reflete exclusivamente os dados presentes na demo atual, sem extrapola√ß√µes e sem misturas com invent√°rio, stocks ou outros m√≥dulos.'"
            else:
                stricter_prompt = full_prompt + "\n\n‚ö†Ô∏è REGRAS ABSOLUTAS:\n- Usa APENAS SKUs e recursos que est√£o explicitamente no contexto JSON.\n- N√ÉO inventes n√∫meros, recursos ou SKUs.\n- N√ÉO mistures m√≥dulos.\n- N√ÉO uses frases gen√©ricas sem n√∫meros."
            try:
                answer = llm.generate(prompt=stricter_prompt, temperature=temperature * 0.7, max_tokens=max_tokens, num_ctx=num_ctx).strip()
                # Re-validar
                validation = validate_llm_output(answer, context, mode, valid_skus, valid_resources)
                industrial_validation = industrial_validator.validate(
                    answer, context, mode, valid_skus, valid_resources, action_candidates
                )
                all_errors = validation.get("errors", []) + industrial_validation.get("errors", [])
                is_valid = validation.get("is_valid", False) and industrial_validation.get("is_valid", False)
                should_regenerate = validation.get("should_regenerate", False) or industrial_validation.get("should_regenerate", False)
            except:
                pass
        
        # Usar texto sanitizado do validador industrial (mais rigoroso)
        final_answer = industrial_validation.get("sanitized_text") or validation.get("sanitized_text") or answer
        
        # Se ainda houver problemas cr√≠ticos ap√≥s regenera√ß√£o, invalidar cache e n√£o guardar
        if not is_valid and (should_regenerate or len(all_errors) > 0):
            # Invalidar cache para for√ßar regenera√ß√£o na pr√≥xima vez
            cache.invalidate(batch_id, mode)
            logger.warning(f"Resumo de {mode} n√£o passou na valida√ß√£o. Cache invalidado. Erros: {all_errors}")
            # Ainda assim devolver o texto sanitizado, mas marcar como inv√°lido
            final_answer = f"[AVISO: Resumo n√£o passou na valida√ß√£o. Erros: {', '.join(all_errors[:3])}] {final_answer}"
        
        # Guardar no cache apenas se passar na valida√ß√£o e n√£o houver user_message
        if not user_message and is_valid and not should_regenerate:
            cache.set(batch_id, mode, final_answer, metadata={"mode": mode, "batch_id": batch_id})

        return {
            "mode": mode,
            "answer": final_answer,
            "insight": final_answer,  # Campo adicional para compatibilidade com frontend
            "model": getattr(llm, "model_name", "llama3:8b"),
            "batch_id": batch_id,
            "cached": False,
            "context": context,
        }
    except HTTPException:
        # Re-raise HTTPExceptions (j√° t√™m status code apropriado)
        raise
    except Exception as exc:
        logger.exception(f"Falha ao gerar insight para {mode}")
        # Retornar mensagem de erro mais amig√°vel
        error_detail = str(exc)
        if "LLMUnavailableError" in error_detail or "Ollama" in error_detail:
            return JSONResponse({"detail": "Modelo offline ‚Äî iniciar Ollama."}, status_code=200)
        raise HTTPException(status_code=500, detail=f"Erro ao gerar insight: {error_detail}") from exc


@router.delete("/cache/{batch_id}")
async def invalidate_cache(batch_id: str, mode: Optional[str] = Query(None)):
    """Invalida cache de insights para um batch_id (e opcionalmente um mode espec√≠fico)."""
    cache = get_insight_cache()
    cache.invalidate(batch_id, mode)
    return {"ok": True, "message": f"Cache invalidado para batch_id={batch_id}, mode={mode or 'todos'}"}

